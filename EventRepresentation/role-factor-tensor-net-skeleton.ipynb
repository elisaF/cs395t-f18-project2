{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role-Factor Tensor Network\n",
    "\n",
    "* Paper: [Weber et al. (2018). Event Representations with Tensor-based Compositions](https://arxiv.org/pdf/1711.07611.pdf)\n",
    "* PyTorch version: 0.4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate random event data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "\n",
    "SUBJ_ANIMAL = DOBJ_ANIMAL = [\"cat\", \"horse\", \"dog\"]\n",
    "SUBJ_HUMAN = DOBJ_HUMAN = [\"man\", \"woman\", \"phd\"]\n",
    "PRED_ANIMAL = [\"eat\", \"bite\", \"poop\"]\n",
    "PRED_HUMAN = [\"read\", \"love\", \"miss\"]\n",
    "\n",
    "class Indexer(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.item_to_index = dict()\n",
    "        self.index_to_item = dict()\n",
    "        self.item_to_count = Counter()\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return \"The size of the indexer = %d\" % len(self.item_to_index)\n",
    "    \n",
    "    def get_item(self, index):\n",
    "        if index in self.index_to_item:\n",
    "            return self.index_to_item[index]\n",
    "        return -1\n",
    "    \n",
    "    def get_index(self, item):\n",
    "        if item not in self.item_to_index:\n",
    "            index = len(self.item_to_index)\n",
    "            self.item_to_index[item] = index\n",
    "            self.index_to_item[index] = item\n",
    "            self.item_to_count[item] += 1\n",
    "        return self.item_to_index[item] \n",
    "    \n",
    "    def get_count(self, item):\n",
    "        if item in self.item_to_count:\n",
    "            return self.item_to_count[item]\n",
    "        return 0\n",
    "    \n",
    "    def get_top_item_set(self, k):\n",
    "        return set(item for item, count in self.item_to_count.most_common(k))\n",
    "    \n",
    "    def get_top_index_set(self, k):\n",
    "        return set(self.get_index(item) \n",
    "                   for item, count in self.item_to_count.most_common(k))\n",
    "     \n",
    "\n",
    "def generate_event_triples(word_indexer, event_indexer, subjects, predicates, dobjects):\n",
    "    events = []\n",
    "    for subject, predicate, dobject in product(subjects, predicates, dobjects):\n",
    "        subject_index = word_indexer.get_index(subject)\n",
    "        predicate_index = word_indexer.get_index(predicate)\n",
    "        dobject_index = word_indexer.get_index(dobject)\n",
    "        event = (subject_index, predicate_index, dobject_index)\n",
    "        events.append(event)\n",
    "        event_indexer.get_index(event)\n",
    "    return np.array(events)\n",
    "\n",
    "word_indexer = Indexer()\n",
    "event_indexer = Indexer()\n",
    "events_list = []\n",
    "for subjects, predicates, dobjects in product([SUBJ_ANIMAL, SUBJ_HUMAN],\n",
    "                                              [PRED_ANIMAL, PRED_HUMAN],\n",
    "                                              [DOBJ_ANIMAL, DOBJ_HUMAN]):\n",
    "    events_list.append(generate_event_triples(word_indexer, event_indexer,\n",
    "                                              subjects, predicates, dobjects))\n",
    "    \n",
    "def sample_inner_random_events(events, sample_size):\n",
    "    random_indices = np.random.choice(range(len(events)),\n",
    "                                      size=sample_size)\n",
    "    return np.array([events[index] for index in random_indices])\n",
    "\n",
    "def sample_outer_random_events(event_indexer, sample_size):\n",
    "    random_indices = np.random.choice(range(len(event_indexer.index_to_item)),\n",
    "                                      size=sample_size)\n",
    "    return np.array([event_indexer.get_item(index) for index in random_indices])\n",
    "\n",
    "def get_batch(event_indexer, events, batch_size=10):\n",
    "    batch_events = sample_inner_random_events(events, batch_size)\n",
    "    batch_positive = sample_inner_random_events(events, batch_size)\n",
    "    batch_negative = sample_outer_random_events(event_indexer, batch_size)\n",
    "    return batch_events, batch_positive, batch_negative\n",
    "\n",
    "def event_to_string(word_indexer, event):\n",
    "    return \"-\".join([word_indexer.get_item(event_index) for event_index in event])\\\n",
    "\n",
    "def event_to_integer(word_indexer, event):\n",
    "    return np.array([word_indexer.get_index(word) for word in event])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role-Factor Tensor Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch.autograd import Variable as Var\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class Embeddings(nn.Module):\n",
    "    \"\"\"Embedding lookup table.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_size):\n",
    "        \"\"\"Initializer.\n",
    "        \n",
    "        Args:\n",
    "            vocab_size: size of input vocabulary (max index - 1).\n",
    "            embedding_size: embedding size.\n",
    "        \"\"\"\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "    \n",
    "    def info(self):\n",
    "        \"\"\"Print out embedding lookup table stats.\"\"\"\n",
    "        print(\"Embedding lookup table of size <%d, %d>\" % (self.vocab_size,\n",
    "                                                           self.embedding_size))\n",
    "    \n",
    "    def forward(self, batch_inputs):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            batch_inputs: Variable.Tensor of the shape \n",
    "                          <batch_size, ..., sequence length>.\n",
    "        Returns:\n",
    "            Variable.Tensor of the shape <batch_size, ..., sequence length, embedding_size>.\n",
    "        \"\"\"\n",
    "        return self.embed(batch_inputs) * math.sqrt(self.embedding_size)\n",
    "\n",
    "\n",
    "class RoleFactorTensorNet(nn.Module):\n",
    "    \"\"\"Role-Factor Tensor Net (Weber et al. 2018).\"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_size, hidden_size, output_size):\n",
    "        \"\"\"Initializer.\n",
    "        \n",
    "        Args:\n",
    "            embedding_size: word embedding size.\n",
    "            hidden_size: model hidden size.\n",
    "            output_size: event embedding size.\n",
    "        \"\"\"\n",
    "        super(RoleFactorTensorNet, self).__init__()\n",
    "        self.T = torch.FloatTensor(hidden_size, embedding_size, embedding_size)\n",
    "        nn.init.xavier_uniform_(self.T)\n",
    "        self.W1 = nn.Linear(hidden_size, output_size)\n",
    "        self.W2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, batch_subjects, batch_predicates, batch_dobjects):\n",
    "        \"\"\"Forward pass.\n",
    "        \n",
    "        Args:\n",
    "            batch_subjects: batch of subject embeddings, \n",
    "                            shape <batch_size, embedding_size>.\n",
    "            batch_predicates: same as batch_subjects, for predicates.\n",
    "            batch_dobjects: same as batch_subjects, for direct objects.\n",
    "        Returns:\n",
    "            Batch of event embeddings, shape <batch_size, output_size>.\n",
    "        \"\"\"\n",
    "        # Einstein sum for tensor contraction (Weber/18, Eq.6).\n",
    "        batch_v_subjects = torch.einsum(\"ijk,bj,bk->bi\", [self.T, \n",
    "                                                          batch_subjects,\n",
    "                                                          batch_predicates])\n",
    "        batch_v_dobjects = torch.einsum(\"ijk,bj,bk->bi\", [self.T, \n",
    "                                                          batch_dobjects,\n",
    "                                                          batch_predicates])\n",
    "        # Argument composition through linear layer (Weber/18, Eq.7).\n",
    "        return self.W1(batch_v_subjects) + self.W2(batch_v_dobjects)\n",
    "\n",
    "\n",
    "def compose_event(batch_inputs, embedder, rft_net):\n",
    "    \"\"\"Input->Event composition with Role-Factor Tensor Net.\n",
    "    \n",
    "    Args:\n",
    "        batch_inputs: numpy ndarray, shape = <batch_size, svo=3>.\n",
    "        embedder: Embeddings object.\n",
    "        rft_net: RoleFactorTensorNet object.\n",
    "    Returns:\n",
    "        Compositional event embedding, shape = <batch_size, event_size>.\n",
    "    \"\"\"\n",
    "    batch_subjects = embedder(torch.LongTensor(batch_inputs[:, 0]))\n",
    "    batch_predicates = embedder(torch.LongTensor(batch_inputs[:, 1]))\n",
    "    batch_dobjects = embedder(torch.LongTensor(batch_inputs[:, 2]))\n",
    "    return rft_net(batch_subjects, batch_predicates, batch_dobjects)\n",
    "\n",
    "\n",
    "def compute_event_similarity(event1, event2, word_indexer, embedder, rft_net):\n",
    "    \"\"\"Compute similarity for a pair of events with trained RFT-Net.\n",
    "    \n",
    "    Args:\n",
    "        event1: (subject, predicate, dobject) string tuple.\n",
    "        event2: same as event1.\n",
    "        word_indexer: Indexer object.\n",
    "        embedder: Embeddings object.\n",
    "        rft_net: RoleFactorTensorNet object.\n",
    "    Returns:\n",
    "        Cosine similarity between event1 and event2 (as composed with RFT-Net).\n",
    "    \"\"\"\n",
    "    event1 = event_to_integer(word_indexer, event1)\n",
    "    event2 = event_to_integer(word_indexer, event2)\n",
    "    event1_embedding = compose_event(np.array([event1]), embedder, rft_net)\n",
    "    event2_embedding = compose_event(np.array([event2]), embedder, rft_net)\n",
    "    return F.cosine_similarity(event1_embedding, event2_embedding).item()\n",
    "    \n",
    "\n",
    "def train(word_indexer, event_indexer,\n",
    "          events_list, # a list of <?, svo=3> event batches.\n",
    "          embedding_size, hidden_size, output_size,\n",
    "          number_epochs, batch_size, learning_rate, margin,\n",
    "          print_every):\n",
    "    \n",
    "    rft_net = RoleFactorTensorNet(embedding_size, hidden_size, output_size)\n",
    "    word_embedder = Embeddings(vocab_size=len(word_indexer.index_to_item),\n",
    "                               embedding_size=embedding_size)\n",
    "    \n",
    "    optimizer = optim.Adam(rft_net.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(number_epochs):\n",
    "        print(\"Epoch %d:\\n\" % (epoch+1))\n",
    "        for events in events_list:\n",
    "            # Get batch inputs, shape = <batch_size, svo=3>.\n",
    "            batch_events, batch_positive, batch_negative = get_batch(event_indexer,\n",
    "                                                                     events, \n",
    "                                                                     batch_size)\n",
    "            # Encode svo-triples as event vectors,\n",
    "            #   shape = <batch_size, event_size>.\n",
    "            batch_events = compose_event(batch_events, word_embedder, rft_net)\n",
    "            batch_positive = compose_event(batch_positive, word_embedder, rft_net)\n",
    "            batch_negative = compose_event(batch_negative, word_embedder, rft_net)\n",
    "            \n",
    "            # Compute batch similarity, shape = <batch_size, >.\n",
    "            similarity_positive = F.cosine_similarity(batch_events, batch_positive)\n",
    "            similarity_negative = F.cosine_similarity(batch_events, batch_negative)\n",
    "            similarity_difference = torch.mean(similarity_negative) - torch.mean(similarity_positive)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            # Compute Hinge Loss: mean(sum(max(0.0, margin + sim_neg - sim_pos))).\n",
    "            #   Weber/18, page 3.\n",
    "            loss = torch.max(torch.FloatTensor(np.array(0.0)), \n",
    "                             margin + similarity_difference)\n",
    "            optimizer.step()\n",
    "        print(\"Epoch loss = %.4f\\n\" % loss.item())\n",
    "    \n",
    "    return rft_net, word_embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:\n",
      "\n",
      "Epoch loss = 0.2047\n",
      "\n",
      "Epoch 2:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 3:\n",
      "\n",
      "Epoch loss = 1.1620\n",
      "\n",
      "Epoch 4:\n",
      "\n",
      "Epoch loss = 1.7363\n",
      "\n",
      "Epoch 5:\n",
      "\n",
      "Epoch loss = 1.4145\n",
      "\n",
      "Epoch 6:\n",
      "\n",
      "Epoch loss = 1.2888\n",
      "\n",
      "Epoch 7:\n",
      "\n",
      "Epoch loss = 1.7896\n",
      "\n",
      "Epoch 8:\n",
      "\n",
      "Epoch loss = 0.7008\n",
      "\n",
      "Epoch 9:\n",
      "\n",
      "Epoch loss = 0.2000\n",
      "\n",
      "Epoch 10:\n",
      "\n",
      "Epoch loss = 0.3229\n",
      "\n",
      "Epoch 11:\n",
      "\n",
      "Epoch loss = 0.9304\n",
      "\n",
      "Epoch 12:\n",
      "\n",
      "Epoch loss = 1.3641\n",
      "\n",
      "Epoch 13:\n",
      "\n",
      "Epoch loss = 0.4745\n",
      "\n",
      "Epoch 14:\n",
      "\n",
      "Epoch loss = 0.4743\n",
      "\n",
      "Epoch 15:\n",
      "\n",
      "Epoch loss = 0.6939\n",
      "\n",
      "Epoch 16:\n",
      "\n",
      "Epoch loss = 0.0566\n",
      "\n",
      "Epoch 17:\n",
      "\n",
      "Epoch loss = 0.9931\n",
      "\n",
      "Epoch 18:\n",
      "\n",
      "Epoch loss = 1.1332\n",
      "\n",
      "Epoch 19:\n",
      "\n",
      "Epoch loss = 0.5491\n",
      "\n",
      "Epoch 20:\n",
      "\n",
      "Epoch loss = 0.8481\n",
      "\n",
      "Epoch 21:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 22:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 23:\n",
      "\n",
      "Epoch loss = 1.0441\n",
      "\n",
      "Epoch 24:\n",
      "\n",
      "Epoch loss = 1.3546\n",
      "\n",
      "Epoch 25:\n",
      "\n",
      "Epoch loss = 0.1876\n",
      "\n",
      "Epoch 26:\n",
      "\n",
      "Epoch loss = 0.1612\n",
      "\n",
      "Epoch 27:\n",
      "\n",
      "Epoch loss = 1.3934\n",
      "\n",
      "Epoch 28:\n",
      "\n",
      "Epoch loss = 0.5476\n",
      "\n",
      "Epoch 29:\n",
      "\n",
      "Epoch loss = 0.6355\n",
      "\n",
      "Epoch 30:\n",
      "\n",
      "Epoch loss = 1.0474\n",
      "\n",
      "Epoch 31:\n",
      "\n",
      "Epoch loss = 0.2703\n",
      "\n",
      "Epoch 32:\n",
      "\n",
      "Epoch loss = 0.3844\n",
      "\n",
      "Epoch 33:\n",
      "\n",
      "Epoch loss = 0.0489\n",
      "\n",
      "Epoch 34:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 35:\n",
      "\n",
      "Epoch loss = 1.0302\n",
      "\n",
      "Epoch 36:\n",
      "\n",
      "Epoch loss = 0.1643\n",
      "\n",
      "Epoch 37:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 38:\n",
      "\n",
      "Epoch loss = 0.5086\n",
      "\n",
      "Epoch 39:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 40:\n",
      "\n",
      "Epoch loss = 0.8727\n",
      "\n",
      "Epoch 41:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 42:\n",
      "\n",
      "Epoch loss = 0.8377\n",
      "\n",
      "Epoch 43:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 44:\n",
      "\n",
      "Epoch loss = 0.5356\n",
      "\n",
      "Epoch 45:\n",
      "\n",
      "Epoch loss = 0.6560\n",
      "\n",
      "Epoch 46:\n",
      "\n",
      "Epoch loss = 0.2525\n",
      "\n",
      "Epoch 47:\n",
      "\n",
      "Epoch loss = 1.0732\n",
      "\n",
      "Epoch 48:\n",
      "\n",
      "Epoch loss = 1.3422\n",
      "\n",
      "Epoch 49:\n",
      "\n",
      "Epoch loss = 1.3762\n",
      "\n",
      "Epoch 50:\n",
      "\n",
      "Epoch loss = 0.2544\n",
      "\n",
      "Epoch 51:\n",
      "\n",
      "Epoch loss = 0.3717\n",
      "\n",
      "Epoch 52:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 53:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 54:\n",
      "\n",
      "Epoch loss = 0.6817\n",
      "\n",
      "Epoch 55:\n",
      "\n",
      "Epoch loss = 1.2323\n",
      "\n",
      "Epoch 56:\n",
      "\n",
      "Epoch loss = 0.2778\n",
      "\n",
      "Epoch 57:\n",
      "\n",
      "Epoch loss = 0.0420\n",
      "\n",
      "Epoch 58:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 59:\n",
      "\n",
      "Epoch loss = 0.6558\n",
      "\n",
      "Epoch 60:\n",
      "\n",
      "Epoch loss = 0.3151\n",
      "\n",
      "Epoch 61:\n",
      "\n",
      "Epoch loss = 0.6317\n",
      "\n",
      "Epoch 62:\n",
      "\n",
      "Epoch loss = 0.4495\n",
      "\n",
      "Epoch 63:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 64:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 65:\n",
      "\n",
      "Epoch loss = 1.0755\n",
      "\n",
      "Epoch 66:\n",
      "\n",
      "Epoch loss = 0.5382\n",
      "\n",
      "Epoch 67:\n",
      "\n",
      "Epoch loss = 0.1205\n",
      "\n",
      "Epoch 68:\n",
      "\n",
      "Epoch loss = 0.9748\n",
      "\n",
      "Epoch 69:\n",
      "\n",
      "Epoch loss = 0.5443\n",
      "\n",
      "Epoch 70:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 71:\n",
      "\n",
      "Epoch loss = 0.9085\n",
      "\n",
      "Epoch 72:\n",
      "\n",
      "Epoch loss = 0.8332\n",
      "\n",
      "Epoch 73:\n",
      "\n",
      "Epoch loss = 1.3591\n",
      "\n",
      "Epoch 74:\n",
      "\n",
      "Epoch loss = 0.9769\n",
      "\n",
      "Epoch 75:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 76:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 77:\n",
      "\n",
      "Epoch loss = 0.1044\n",
      "\n",
      "Epoch 78:\n",
      "\n",
      "Epoch loss = 0.4706\n",
      "\n",
      "Epoch 79:\n",
      "\n",
      "Epoch loss = 0.6659\n",
      "\n",
      "Epoch 80:\n",
      "\n",
      "Epoch loss = 0.2446\n",
      "\n",
      "Epoch 81:\n",
      "\n",
      "Epoch loss = 0.3281\n",
      "\n",
      "Epoch 82:\n",
      "\n",
      "Epoch loss = 0.6012\n",
      "\n",
      "Epoch 83:\n",
      "\n",
      "Epoch loss = 0.6180\n",
      "\n",
      "Epoch 84:\n",
      "\n",
      "Epoch loss = 1.6095\n",
      "\n",
      "Epoch 85:\n",
      "\n",
      "Epoch loss = 0.3835\n",
      "\n",
      "Epoch 86:\n",
      "\n",
      "Epoch loss = 1.0988\n",
      "\n",
      "Epoch 87:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 88:\n",
      "\n",
      "Epoch loss = 1.7934\n",
      "\n",
      "Epoch 89:\n",
      "\n",
      "Epoch loss = 0.3940\n",
      "\n",
      "Epoch 90:\n",
      "\n",
      "Epoch loss = 0.5933\n",
      "\n",
      "Epoch 91:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 92:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 93:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n",
      "Epoch 94:\n",
      "\n",
      "Epoch loss = 0.8146\n",
      "\n",
      "Epoch 95:\n",
      "\n",
      "Epoch loss = 1.4772\n",
      "\n",
      "Epoch 96:\n",
      "\n",
      "Epoch loss = 1.3544\n",
      "\n",
      "Epoch 97:\n",
      "\n",
      "Epoch loss = 0.4673\n",
      "\n",
      "Epoch 98:\n",
      "\n",
      "Epoch loss = 0.5740\n",
      "\n",
      "Epoch 99:\n",
      "\n",
      "Epoch loss = 0.2466\n",
      "\n",
      "Epoch 100:\n",
      "\n",
      "Epoch loss = 0.0000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net, emb = train(word_indexer, event_indexer,\n",
    "                 events_list,\n",
    "                 embedding_size=3, hidden_size=4, output_size=5,\n",
    "                 number_epochs=100, batch_size=2, learning_rate=1e-4, margin=0.5,\n",
    "                 print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.611241340637207\n",
      "-0.08810067176818848\n"
     ]
    }
   ],
   "source": [
    "e1 = (\"dog\", \"eat\", \"dog\")\n",
    "e2 = (\"cat\", \"eat\", \"horse\")\n",
    "e3 = (\"man\", \"love\", \"phd\")\n",
    "\n",
    "print(compute_event_similarity(e1, e2, word_indexer, emb, net))\n",
    "print(compute_event_similarity(e1, e3, word_indexer, emb, net))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
